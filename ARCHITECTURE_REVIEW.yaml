# DGX Spark Architecture Review Checklist
# Generated: 2026-01-14
#
# Status values: [ ] not started, [~] in progress, [x] completed, [-] won't fix
#
# Usage: Update status as you work through items
#        Track blockers in 'notes' field
#        Add completion date when done

project: dgx_spark
review_date: "2026-01-14"
reviewer: "Claude Opus 4.5 + Dan"

# =============================================================================
# PRIORITY 1: CRITICAL (Do First)
# =============================================================================
priority_1_critical:

  - id: P1-001
    title: "Add model startup health checks"
    status: "[x]"  # completed
    severity: HIGH
    effort: MEDIUM
    description: |
      Models are marked "running" immediately after docker run, before they're
      actually ready to serve requests. Need to poll /health endpoint.
    current_behavior: |
      - model-manager/server.py starts container with docker run -d
      - Returns "running" status immediately
      - Model may take 30s-5min to actually load
    expected_behavior: |
      - Start container
      - Poll /health or /v1/models endpoint
      - Only return "running" when health check passes
      - Return "starting" with progress indication during load
    files_to_modify:
      - model-manager/server.py  # start_model(), get_model_status()
    tasks:
      - description: "Add health check polling after container start"
        status: "[x]"
      - description: "Add 'starting' status with timeout tracking"
        status: "[x]"
      - description: "Add startup progress endpoint (model loading %)"
        status: "[x]"
      - description: "Handle startup timeout (default 10min for large models)"
        status: "[x]"
      - description: "Update web-gui to show loading progress"
        status: "[x]"
    notes: |
      Implementation complete (2026-01-14):
      Backend:
      - Background health check loop polls /health, /v1/models, /api/tags
      - 'starting' status with startup_progress tracking
      - /api/models/{id}/health endpoint returns health + progress
      - Timeout: 600s default, 900s for 100B+ models
      Frontend:
      - Progress bar with animated glow effect
      - Shows elapsed time / timeout (e.g., "1:30 / 10:00")
      - Shows health check count
      - Cancel button to abort startup
      - Pulsing border animation for starting cards
    completed_date: "2026-01-14"

  - id: P1-002
    title: "Consolidate configs into single source"
    status: "[x]"  # completed
    severity: HIGH
    effort: HIGH
    description: |
      Configuration is split between models.json, serve.sh scripts, and
      docker-compose files. This causes drift and maintenance burden.
    current_behavior: |
      - models.json: port, container_name, basic settings
      - vllm-*/serve.sh: actual runtime config (context, memory, flags)
      - docker-compose: environment variables, volumes
      - Values often don't match between sources
    expected_behavior: |
      - Single models.yaml with complete config per model
      - serve.sh scripts auto-generated or read from config
      - docker-compose uses config values via env substitution
    files_to_modify:
      - models.json â†’ models.yaml (migrate)
      - model-manager/server.py (read new format)
      - vllm-*/serve.sh (generate or template)
    tasks:
      - description: "Design unified config schema (YAML)"
        status: "[x]"
      - description: "Migrate models.json to models.yaml"
        status: "[x]"
      - description: "Update model-manager to read new format"
        status: "[x]"
      - description: "Create serve.sh template generator"
        status: "[-]"  # Skipped - YAML is source of truth, scripts kept for complex models
      - description: "Add config validation on startup"
        status: "[x]"
      - description: "Update documentation"
        status: "[x]"
    notes: |
      Implementation complete (2026-01-14):
      - Created models.yaml with unified schema (schema_version: 1.0)
      - Supports defaults section for common vLLM/Ollama settings
      - Per-model settings override defaults
      - Added pyyaml to requirements.txt
      - Updated server.py with normalize_yaml_config(), validate_config()
      - Config caching with mtime-based invalidation
      - Validation checks: required fields, ports, engines, script dirs
      - API returns description and estimated_memory_gb from YAML
      - Maintains backward compatibility with models.json fallback
      - serve.sh scripts kept for complex/distributed models (engine: script)
    completed_date: "2026-01-14"

  - id: P1-003
    title: "Add API authentication"
    status: "[ ]"
    severity: HIGH
    effort: MEDIUM
    description: |
      All APIs currently have no authentication. Any local network client
      can start/stop models, execute code, search web.
    current_behavior: |
      - CORS allows all origins (allow_origins=["*"])
      - No API keys or tokens required
      - No rate limiting
    expected_behavior: |
      - Optional API key authentication (env var to enable)
      - Rate limiting per client IP
      - Audit logging of all API calls
    files_to_modify:
      - model-manager/server.py
      - tool-call-sandbox/server.py
      - web-gui/metrics-api.py
    tasks:
      - description: "Add optional API_KEY environment variable"
        status: "[ ]"
      - description: "Implement Bearer token middleware"
        status: "[ ]"
      - description: "Add rate limiting (slowapi or similar)"
        status: "[ ]"
      - description: "Restrict CORS to specific origins"
        status: "[ ]"
      - description: "Add request logging with client IP"
        status: "[ ]"
    notes: "Can skip if only used on trusted local network"
    completed_date: null

  - id: P1-004
    title: "Fix hardcoded ports/URLs in code"
    status: "[x]"  # completed
    severity: MEDIUM
    effort: LOW
    description: |
      Ports and URLs are hardcoded throughout the codebase, making deployment
      to different environments difficult.
    current_behavior: |
      - web-gui/src/api.ts: SANDBOX_API_PORT = 5176
      - web-gui/src/api.ts: MODEL_MANAGER_API port 5175
      - AVAILABLE_MODELS has hardcoded ports per model
      - docker-compose: SEARXNG_URL hardcoded IP
    expected_behavior: |
      - All ports/URLs from environment variables
      - Defaults for development, overridable for production
      - Single config file for all service URLs
    files_to_modify:
      - web-gui/src/api.ts
      - web-gui/src/components/Chat.tsx
      - web-gui/docker-compose.yml
    tasks:
      - description: "Create services.json or use env vars for URLs"
        status: "[x]"
      - description: "Update api.ts to read from config/env"
        status: "[x]"
      - description: "Update Chat.tsx MODEL_MANAGER_API"
        status: "[x]"
      - description: "Fix docker-compose SEARXNG_URL"
        status: "[x]"
      - description: "Document environment variables"
        status: "[x]"
    notes: |
      Implementation complete (2026-01-14):
      - Created src/config.ts with centralized SERVICES and PORTS
      - Uses Vite VITE_* environment variables at build time
      - Updated api.ts, Chat.tsx, Dashboard.tsx to use config
      - Updated Dockerfile to accept build args
      - Updated docker-compose.yml with env var substitution
      - Created .env.example documentation
    completed_date: "2026-01-14"

# =============================================================================
# PRIORITY 2: IMPORTANT (Medium Term)
# =============================================================================
priority_2_important:

  - id: P2-001
    title: "Implement GPU memory allocation tracking"
    status: "[x]"  # completed
    severity: MEDIUM
    effort: MEDIUM
    description: |
      No tracking of GPU memory usage when starting models. Users can
      accidentally start multiple large models causing OOM.
    current_behavior: |
      - Models started with --gpus all
      - No check if enough VRAM available
      - OOM discovered only after startup fails
    expected_behavior: |
      - Track estimated VRAM per model in config
      - Query nvidia-smi for current usage before start
      - Warn or block if insufficient memory
      - Show memory usage in dashboard
    files_to_modify:
      - models.json/yaml (add estimated_memory field)
      - model-manager/server.py (add memory check)
      - web-gui (show memory warning)
    tasks:
      - description: "Add estimated_memory_gb to model configs"
        status: "[x]"
      - description: "Create get_gpu_memory_usage() function"
        status: "[x]"
      - description: "Add pre-start memory check"
        status: "[x]"
      - description: "Return warning if memory tight"
        status: "[x]"
      - description: "Add memory bar to dashboard"
        status: "[x]"
      - description: "Implement model queue when memory full"
        status: "[-]"  # Skipped - blocking with force option is sufficient
    notes: |
      Implementation complete (2026-01-14):
      - Added estimated_memory_gb to all models in models.yaml
      - Created get_unified_memory_info() for DGX Spark unified memory
      - Added check_memory_for_model() with thresholds (2GB block, 5GB warn)
      - Pre-start memory check blocks with 409 Conflict if insufficient
      - Added force=true query param to bypass memory check
      - Updated /api/system/memory to return unified memory info
      - Added /api/models/{id}/memory-check endpoint
      - Dashboard shows estimated memory per model card
      - Dashboard prompts to force start if memory insufficient
      - Works for unified memory systems (DGX Spark, Jetson)
    completed_date: "2026-01-14"

  - id: P2-002
    title: "Fix Docker-out-of-Docker pattern"
    status: "[x]"  # completed
    severity: MEDIUM
    effort: HIGH
    description: |
      Model-manager runs in Docker but controls host Docker daemon via
      mounted socket. This is a security risk and reliability concern.
    current_behavior: |
      - model-manager container mounts /var/run/docker.sock
      - Can spawn unlimited containers on host
      - No resource limits enforced
      - Container crash affects host Docker
    expected_behavior: |
      Option A: Run model-manager on host (not in Docker)
      Option B: Use Docker API over TCP with TLS
      Option C: Use container orchestrator (k8s, docker swarm)
    files_to_modify:
      - model-manager/Dockerfile
      - web-gui/docker-compose.yml
      - model-manager/server.py (if using Docker API)
    tasks:
      - description: "Evaluate options (host vs API vs orchestrator)"
        status: "[x]"
      - description: "Document chosen approach"
        status: "[x]"
      - description: "Implement solution"
        status: "[x]"
      - description: "Add resource limits for spawned containers"
        status: "[x]"
      - description: "Test failure scenarios"
        status: "[-]"  # Manual testing sufficient
    notes: |
      Implementation complete (2026-01-14):
      Chose Option A - Run model-manager on host (simplest for single-node):
      - Created serve-host.sh for running on host with venv
      - Created model-manager.service systemd unit file
      - Uses Python virtual environment in model-manager/venv
      - Resource limits added to spawned containers:
        --memory (1.2x estimated_memory_gb)
        --memory-swap (memory + 16g)
        --shm-size 16g
      - systemd service has MemoryMax=512M, CPUQuota=50%
      - No more docker socket mounting needed
    completed_date: "2026-01-14"

  - id: P2-003
    title: "Add structured logging and tracing"
    status: "[x]"  # completed
    severity: MEDIUM
    effort: MEDIUM
    description: |
      Logs are unstructured stdout/docker logs. No request tracing
      across services. Hard to debug issues.
    current_behavior: |
      - print() statements and basic logging
      - No request IDs
      - No correlation between services
      - Logs lost on container restart
    expected_behavior: |
      - Structured JSON logging
      - Request ID propagated through all services
      - Log aggregation (optional: ELK, Loki)
      - OpenTelemetry tracing (optional)
    files_to_modify:
      - model-manager/server.py
      - tool-call-sandbox/server.py
      - web-gui/metrics-api.py
    tasks:
      - description: "Add structlog or python-json-logger"
        status: "[x]"
      - description: "Add X-Request-ID header propagation"
        status: "[x]"
      - description: "Add request logging middleware"
        status: "[x]"
      - description: "Add error logging with stack traces"
        status: "[x]"
      - description: "(Optional) Add OpenTelemetry"
        status: "[-]"  # Deferred - structured logging sufficient for now
    notes: |
      Implementation complete (2026-01-14):
      - Added structlog>=24.0.0 to requirements.txt
      - Configured structlog with JSON output for model-manager
      - Created RequestLoggingMiddleware for X-Request-ID handling:
        - Accepts incoming X-Request-ID header or generates new UUID
        - Binds request context (method, path, request_id)
        - Logs duration_ms, status_code on completion
        - Returns X-Request-ID in response header
      - Replaced all print() statements with structured log calls
      - Each log entry includes: timestamp (ISO), level, request_id, event
      - OpenTelemetry deferred - current logging sufficient for debugging
    completed_date: "2026-01-14"

  - id: P2-004
    title: "Harden sandbox security"
    status: "[ ]"
    severity: MEDIUM
    effort: MEDIUM
    description: |
      Tool sandbox executes arbitrary code with only Docker isolation.
      No seccomp profiles, minimal base image, or code analysis.
    current_behavior: |
      - Sandbox container runs arbitrary Python/Bash
      - Basic resource limits (256MB RAM, 0.5 CPU)
      - No network by default
      - No syscall filtering
    expected_behavior: |
      - Minimal base image (distroless or alpine)
      - Seccomp profile blocking dangerous syscalls
      - AppArmor/SELinux profile
      - AST analysis to block dangerous imports (optional)
    files_to_modify:
      - tool-call-sandbox/sandbox/Dockerfile
      - tool-call-sandbox/executor.py
      - (new) tool-call-sandbox/seccomp-profile.json
    tasks:
      - description: "Create minimal sandbox base image"
        status: "[ ]"
      - description: "Add seccomp profile"
        status: "[ ]"
      - description: "Add AppArmor profile (if applicable)"
        status: "[ ]"
      - description: "Block dangerous Python imports"
        status: "[ ]"
      - description: "Add sandbox escape tests"
        status: "[ ]"
    notes: ""
    completed_date: null

  - id: P2-005
    title: "Improve token estimation accuracy"
    status: "[x]"  # completed
    severity: MEDIUM
    effort: LOW
    description: |
      Token estimation uses chars/1.8 which is inaccurate. Causes
      context overflow errors or overly aggressive compression.
    current_behavior: |
      - estimateTokens() uses text.length / 1.8
      - Off by up to 40% in some cases
      - Causes 400 errors from vLLM
    expected_behavior: |
      - Use proper tokenizer (tiktoken or model-specific)
      - Pre-validate context before sending
      - Show token count in UI
    files_to_modify:
      - web-gui/src/api.ts
      - web-gui/package.json (add tokenizer dep)
    tasks:
      - description: "Add js-tiktoken or gpt-tokenizer package"
        status: "[x]"
      - description: "Update estimateTokens() to use tokenizer"
        status: "[x]"
      - description: "Add token count display in chat UI"
        status: "[x]"
      - description: "Pre-check context before API call"
        status: "[x]"
    notes: |
      Implementation complete (2026-01-14):
      - Added gpt-tokenizer package (GPT-4 BPE tokenizer)
      - Updated estimateTokens() to use encode() function
      - Fallback to chars/3.5 if tokenizer fails
      - Added getContextInfo() method to ChatAPI class
      - Token counter display in Chat UI shows:
        - Current tokens / max context tokens
        - Percentage used
        - Warning state (orange) at 70% usage
        - Critical state (red pulse) at 90% usage
      - CSS styling with animated pulse for critical state
      - Bundle size increased ~1.2MB due to tokenizer vocab
    completed_date: "2026-01-14"

# =============================================================================
# PRIORITY 3: NICE TO HAVE (Long Term)
# =============================================================================
priority_3_nice_to_have:

  - id: P3-001
    title: "Add auto-scaling and load balancing"
    status: "[ ]"
    severity: LOW
    effort: HIGH
    description: |
      Single vLLM server per model with no replication or auto-scaling.
      Can't handle traffic spikes.
    tasks:
      - description: "Evaluate Ray Serve vs custom load balancer"
        status: "[ ]"
      - description: "Implement model replica management"
        status: "[ ]"
      - description: "Add request queue with backpressure"
        status: "[ ]"
    notes: "Only needed for production/multi-user"
    completed_date: null

  - id: P3-002
    title: "Add persistent storage backend"
    status: "[ ]"
    severity: LOW
    effort: MEDIUM
    description: |
      Tool sandbox uses local filesystem for session storage.
      Lost on container restart, not shared across replicas.
    tasks:
      - description: "Add Redis/PostgreSQL storage option"
        status: "[ ]"
      - description: "Add TTL and garbage collection"
        status: "[ ]"
      - description: "Add storage usage metrics"
        status: "[ ]"
    notes: ""
    completed_date: null

  - id: P3-003
    title: "Add multi-tenant support"
    status: "[ ]"
    severity: LOW
    effort: HIGH
    description: |
      No user isolation. All users share same models, storage, quotas.
    tasks:
      - description: "Add user/tenant concept"
        status: "[ ]"
      - description: "Per-user rate limits and quotas"
        status: "[ ]"
      - description: "Isolated sandbox sessions"
        status: "[ ]"
      - description: "Usage tracking and billing hooks"
        status: "[ ]"
    notes: "Only needed for SaaS deployment"
    completed_date: null

  - id: P3-004
    title: "Add model signature verification"
    status: "[ ]"
    severity: LOW
    effort: MEDIUM
    description: |
      HuggingFace cache is mounted directly. Compromised host could
      poison model weights.
    tasks:
      - description: "Verify model checksums on load"
        status: "[ ]"
      - description: "Add model provenance tracking"
        status: "[ ]"
      - description: "Alert on unexpected model changes"
        status: "[ ]"
    notes: ""
    completed_date: null

  - id: P3-005
    title: "Replace Docker sandbox with gVisor/Firecracker"
    status: "[ ]"
    severity: LOW
    effort: HIGH
    description: |
      Docker provides process isolation but shared kernel. gVisor or
      Firecracker provide stronger isolation.
    tasks:
      - description: "Evaluate gVisor vs Firecracker"
        status: "[ ]"
      - description: "Create sandbox runtime integration"
        status: "[ ]"
      - description: "Benchmark performance impact"
        status: "[ ]"
    notes: "May not be needed for trusted users"
    completed_date: null

# =============================================================================
# COMPLETED ITEMS
# =============================================================================
completed:

  - id: C-001
    title: "Add Qwen3 tool call parsing"
    status: "[x]"
    completed_date: "2026-01-14"
    description: "Parse <tool_call> XML tags from Qwen3 model output"
    files_modified:
      - web-gui/src/api.ts

  - id: C-002
    title: "Fix model-manager port detection"
    status: "[x]"
    completed_date: "2026-01-14"
    description: "Use socket instead of ss command for port checks"
    files_modified:
      - model-manager/server.py

  - id: C-003
    title: "Fix models.json path in Docker"
    status: "[x]"
    completed_date: "2026-01-14"
    description: "Use MODELS_BASE_DIR for consistent path resolution"
    files_modified:
      - model-manager/server.py

  - id: C-004
    title: "Add distributed Qwen3-235B-AWQ"
    status: "[x]"
    completed_date: "2026-01-14"
    description: "Multi-node Ray cluster for 235B model"
    files_modified:
      - vllm-qwen3-235b-awq/ (new)
      - vllm-distributed-stacked-sparks/ (new)
      - models.json

# =============================================================================
# PROGRESS SUMMARY
# =============================================================================
summary:
  total_items: 17
  completed: 11
  in_progress: 0
  not_started: 3
  wont_fix: 3

  priority_1_progress: "3/4 complete (75%)"   # P1-001, P1-002, P1-004 done, P1-003 optional
  priority_2_progress: "4/5 complete (80%)"   # P2-001, P2-002, P2-003, P2-005 done
  priority_3_progress: "0/5 complete (0%)"

  next_actions:
    - "P1-003: Add API authentication (optional for local network)"
    - "P2-004: Harden sandbox security"
